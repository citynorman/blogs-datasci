---
title: "5 step guide to scalable deep learning pipelines"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(reticulate)
library(kableExtra)

setwd("PATH")
source_python("flow_tasks.py")

```

*Use pytorch and d6tflow on a case study using Facebook deep recommender model.*

# Introduction: Why bother?

Building deep learning models involves a lot of trial and error, tweaking model architecture and parameters whose performance needs to be compared. It is often difficult to keep track of all the experiments, leading at best to confusion and at worst wrong conclusions.

In [4 reasons why your ML code is bad] we learned how to organize ML code as DAGs to solve that problem. In this guide we will go through a practical case study on turning a pytorch script into a scalable deep learning pipeline. The starting point is a pytorch deep recommender model by Facebook. Why that? Great innovation but difficult to understand how code worked and difficult to keep track of parameters.

With a model as complex as Facebook's deep learning recommender, the number of parameters to manage for even a single run is enormous. Moreover, tracking the use of said parameters can be an exercise in futility, particularly if a single paramter influences multiple facets of the model execution. Fortunately, d6tflow allows for easy management of these parameters and much more.

## Step 1: Plan your DAG

* Think about data flow and dependencies between steps
* Organize workflow into logical components
* Help others understand how your pipeline fits together
* Encapsulate your code into modules or nodes for greater flexibility

Below is the DAG for FB DLRM. It is relatively linear, typically you would have more complex dependencies especially if you don't start with a linear workflow. In many ways DAGs act as excellent catalogues for managing and efficiently integrating new functionality. Because of their modularity, developers can often avoid redundant or conflicting code as projects grow.

Even better, few tools allow developers to get a snapshot of their intended execution before compile time, if ever. The functionality of d6tflow not only gives the user a full-view of what their pipeline will do, it can further offer a glimpse into the parameters that will effect the experiment. Not a bad feature for debugging or demonstrations.

```{python}
task = TaskRunDLRMExperiment()
print(d6tflow.preview(task))

```

## Step 2: Write Tasks instead of functions

* Tasks make up the DAG. Can define dependencies and automatically persist intermediary output
* Dont want to rerun every step of the workflow every you time you run it, especially long-running training tasks. Eg Persist trained models and preprocessed data
* In the spirit of seperating code from data, output is saved to d6tpipe
* For repeated runs of similar experiments, nodes are often intelligently re-used, potentially saving lots of time with large or complex pipelines. 

The task automatically saves model output and therefore does not rerun if the model has already been trained. As a Data Scientist in industry or academia, this efficiency DAGs provide has a direct impact on productivity. 

Furthermore, many Data Scientists and engineers readily present their approach and finding to unfamiliar audiences. Functions by themselves can often obscure a general concept, with many contributing in conjunction to accomplish a single goal. Therefore, explanation at a function-level is often more difficult than simply pointing to a neatly displayed task graph and walking through the logical process step by step. In this way, d6tflow acts a both a developer's productivity tool as well as a means for efficient presentation.

```{python, echo=TRUE, eval = FALSE}
# before

def train_model():
    dlrm = DLRM_Net([...])
    torch.save({dlrm},'model.pickle')

if __name__ == "__main__":
    dlrm = torch.load('model.pickle')


# after
class TaskBuildNetwork(d6tflow.tasks.TaskPickle):

    def requires(self):
        return TaskLintParameters()

    def run(self):

        dataset_dict = self.input().load()

        dlrm = DLRM_Net([...])

        self.save(dlrm)

```


## Step 3: Parameterize tasks

* Avoid inadvertant retraining, automatically add
* help others understand where params go and where in pipeline they are introduced
* Visualize the 

Below sets up the task with parameters. You will see at the model comparison stage how this is useful.

```{python, echo=TRUE, eval = FALSE}
# before

if __name__ == "__main__":
    parser.add_argument("--sync-dense-params", type=bool, default=True)
    dlrm = DLRM_Net(
        sync_dense_params=args.sync_dense_params
    )

# after

class TaskBuildNetwork(d6tflow.tasks.TaskPickle):

    sync_dense_params = luigi.BoolParameter(default = True)

    def run(self):
        dlrm = DLRM_Net(
            sync_dense_params=self.sync_dense_params
        )

```


### Inherit parameters

* parameter automatically cascades through workflow
* run final task with parameters
* quick to compare models after DAG is run

`TaskRunDLRMExperiment` inherits parameters from `TaskBuildNetwork`. This way you can run `TaskRunDLRMExperiment(sync_dense_params=False)` and it will pass the parameter to upstream tasks ie `TaskBuildNetwork` and all other tasks that depend on it.

Having tasks inherit parameters selectively from its parents has two major benefits. For one, developers no longer have to worry about passing the same parameters between functions manually. DAG inheritance with d6tflow offers a seamless and memory efficient method of parameter persistance.

Second, with all parameter mapping now abstracted, creating and executing full pipelines is as simple as generating a collection of parameters, either in a configuration file or explicit distionary, and running the terminal task of the DAG. Best of all, users can set default parameters in the configuration and then selectively override them based on specific experiments. No longer do developers need to explicitly define every parameter for pipeline experiments.

```{python, echo=TRUE, eval = FALSE}

class TaskBuildNetwork(d6tflow.tasks.TaskPickle):

    sync_dense_params = luigi.BoolParameter(default = True)
    # [...]

@d6tflow.inherit(TaskBuildNetwork)
@d6tflow.clone_parent()
class TaskRunDLRMExperiment(d6tflow.tasks.TaskPickle):
    # [...]
    pass

```

## Step 4: Run DAG to train model

* preview pipeline, check flow is correct
* execute, automatically runs all dependencies including any preprocessing and training tasks

As the terminal node for the Facebook deep learning recommender, `TaskRunDLRMExperiment` is the only task that needs to be defined and run for the entire pipeline to execute. No need for main methods full of function-specific code!

```{python, eval = FALSE}
task = TaskRunDLRMExperiment()
d6tflow.run(task))

```


## Step 5: Test performance

* once all tasks are complete, you can load predictions and other model output
* run diagnostics as usual

Finally, we can take advantage of d6tpipe's data persistence and easiy assess the performance of our models.

```{python, eval = FALSE}
model1 = TaskRunDLRMExperiment().output().load()
print_accuracy(model1)

```


### Compare models

* load output from different models using parameters

Every model needs a benchmark for comparison, and DAGs like d6tflow make it easy to quickly compare and contrast the performance of different models. By saving artifacts of the execution to d6tpipe and calling them back later for future use, there is never a concern for losing precious data mid-execution. Halted executions no longer mean a complete loss of all data.

```{python, eval = FALSE}
model1 = TaskRunDLRMExperiment().output().load()
print_accuracy(model1)
model2 = TaskRunDLRMExperiment(sync_dense_params=False).output().load()
print_accuracy(model2)

```

## Keep iterating

* invalidate previous experiments
* change parameters
* automatically knows which tasks need to be run

For example, say you changed training data or made changes to the training preprocessing.Instead of rolling back all of the previous information gathered from other executions, you can manually set which nodes you would like to invalidate. The dynamic nature of 

```{python, eval = FALSE}

TaskGetTrainDataset().invalidate()

# or
d6tflow.run(task, forced=TaskGetTrainDataset())


```

## Try yourself

All code is provided at https://github.com/d6tdev/dlrm

* flow_run.py: run flow => this is the file you want to run
* flow_task.py: task code
* flow_cfg.py: parameters


## Your next project

In this guide we showed how to build scalable deep learning pipelines. We used an existing code base to explain how to turn linear deep learning code into DAGs and the benefits of doing so.

For new projects, you can start with a clean project template from https://github.com/d6t/d6tflow-template. The structure is very similar:

* run.py: run workflow
* task.py: task code
* cfg.py: manage parameters

